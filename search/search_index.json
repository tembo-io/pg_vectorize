{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"pg_vectorize: a VectorDB for Postgres A Postgres extension that automates the transformation and orchestration of text to embeddings and provides hooks into the most popular LLMs. This allows you to do vector search and build LLM applications on existing data with as little as two function calls. This project relies heavily on the work by pgvector for vector similarity search, pgmq for orchestration in background workers, and SentenceTransformers . pg_vectorize powers the VectorDB Stack on Tembo Cloud and is available in all hobby tier instances. API Documentation : https://tembo.io/pg_vectorize/ Source : https://github.com/tembo-io/pg_vectorize Features \u00b6 Workflows for both vector search and RAG Integrations with OpenAI's embeddings and Text-Generation endpoints and a self-hosted container for running Hugging Face Sentence-Transformers Automated creation of Postgres triggers to keep your embeddings up to date High level API - one function to initialize embeddings transformations, and another function to search Table of Contents \u00b6 Features Table of Contents Installation Vector Search Example RAG Example Updating Embeddings Directly Interact with LLMs Importing Pre-existing Embeddings Creating a Table from Existing Embeddings Installation \u00b6 The fastest way to get started is by running the Tembo docker container and the vector server with docker compose: docker compose up -d Then connect to Postgres: docker compose exec -it postgres psql Enable the extension and its dependencies CREATE EXTENSION vectorize CASCADE ; Install into an existing Postgres instance If you're installing in an existing Postgres instance, you will need the following dependencies: Rust: - [pgrx toolchain](https://github.com/pgcentralfoundation/pgrx) Postgres Extensions: - [pg_cron](https://github.com/citusdata/pg_cron) ^1.5 - [pgmq](https://github.com/tembo-io/pgmq) ^1 - [pgvector](https://github.com/pgvector/pgvector) ^0.5.0 Then set the following either in postgresql.conf or as a configuration parameter: -- requires restart of Postgres alter system set shared_preload_libraries = 'vectorize,pg_cron' ; alter system set cron . database_name = 'postgres' ; And if you're running the vector-serve container, set the following url as a configuration parameter in Postgres. The host may need to change from `localhost` to something else depending on where you are running the container. alter system set vectorize . embedding_service_url = 'http://localhost:3000/v1' ; SELECT pg_reload_conf (); Vector Search Example \u00b6 Text-to-embedding transformation can be done with either Hugging Face's Sentence-Transformers or OpenAI's embeddings. The following examples use Hugging Face's Sentence-Transformers. See the project documentation for OpenAI examples. Follow the installation steps if you haven't already. Setup a products table. Copy from the example data provided by the extension. CREATE TABLE products ( LIKE vectorize . example_products INCLUDING ALL ); INSERT INTO products SELECT * FROM vectorize . example_products ; SELECT * FROM products limit 2 ; product_id | product_name | description | last_updated_at ------------+--------------+--------------------------------------------------------+------------------------------- 1 | Pencil | Utensil used for writing and often works best on paper | 2023-07-26 17:20:43.639351-05 2 | Laptop Stand | Elevated platform for laptops, enhancing ergonomics | 2023-07-26 17:20:43.639351-05 Create a job to vectorize the products table. We'll specify the tables primary key (product_id) and the columns that we want to search (product_name and description). SELECT vectorize . table ( job_name => 'product_search_hf' , relation => 'products' , primary_key => 'product_id' , columns => ARRAY [ 'product_name' , 'description' ], transformer => 'sentence-transformers/all-MiniLM-L6-v2' , schedule => 'realtime' ); This adds a new column to your table, in our case it is named product_search_embeddings , then populates that data with the transformed embeddings from the product_name and description columns. Then search, SELECT * FROM vectorize . search ( job_name => 'product_search_hf' , query => 'accessories for mobile devices' , return_columns => ARRAY [ 'product_id' , 'product_name' ], num_results => 3 ); search_results --------------------------------------------------------------------------------------------- {\"product_id\": 13, \"product_name\": \"Phone Charger\", \"similarity_score\": 0.8147814132322894} {\"product_id\": 6, \"product_name\": \"Backpack\", \"similarity_score\": 0.7743061352550308} {\"product_id\": 11, \"product_name\": \"Stylus Pen\", \"similarity_score\": 0.7709902653575383} RAG Example \u00b6 Ask raw text questions of the example products dataset and get chat responses from an OpenAI LLM. Follow the installation steps if you haven't already. Set the OpenAI API key , this is required to for use with OpenAI's chat-completion models. ALTER SYSTEM SET vectorize . openai_key TO '<your api key>' ; SELECT pg_reload_conf (); Create an example table if it does not already exist. CREATE TABLE products ( LIKE vectorize . example_products INCLUDING ALL ); INSERT INTO products SELECT * FROM vectorize . example_products ; Initialize a table for RAG. We'll use an open source Sentence Transformer to generate embeddings. Create a new column that we want to use as the context. In this case, we'll concatenate both product_name and description . ALTER TABLE products ADD COLUMN context TEXT GENERATED ALWAYS AS ( product_name || ': ' || description ) STORED ; Initialize the RAG project. We'll use the openai/text-embedding-3-small model to generate embeddings on our source documents. SELECT vectorize . table ( job_name => 'product_chat' , relation => 'products' , primary_key => 'product_id' , columns => ARRAY [ 'context' ], transformer => 'openai/text-embedding-3-small' , schedule => 'realtime' ); Now we can ask questions of the products table and get responses from the product_chat agent using the openai/gpt-3.5-turbo generative model. SELECT vectorize . rag ( job_name => 'product_chat' , query => 'What is a pencil?' , chat_model => 'openai/gpt-3.5-turbo' ) -> 'chat_response' ; \"A pencil is an item that is commonly used for writing and is known to be most effective on paper.\" And to use a locally hosted Ollama service, change the chat_model parameter: SELECT vectorize . rag ( job_name => 'product_chat' , query => 'What is a pencil?' , chat_model => 'ollama/wizardlm2:7b' ) -> 'chat_response' ; \" A pencil is a writing instrument that consists of a solid or gelignola wood core, known as the \\\"lead,\\\" encased in a cylindrical piece of breakable material (traditionally wood or plastic), which serves as the body of the pencil. The tip of the body is tapered to a point for writing, and it can mark paper with the imprint of the lead. When used on a sheet of paper, the combination of the pencil's lead and the paper creates a visible mark that is distinct from unmarked areas of the paper. Pencils are particularly well-suited for writing on paper, as they allow for precise control over the marks made.\" :bulb: Note that the -> 'chat_response' addition selects for that field of the JSON object output. Removing it will show the full JSON object, including information on which documents were included in the contextual prompt. Updating Embeddings \u00b6 When the source text data is updated, how and when the embeddings are updated is determined by the value set to the schedule parameter in vectorize.table . The default behavior is schedule => '* * * * *' , which means the background worker process checks for changes every minute, and updates the embeddings accordingly. This method requires setting the updated_at_col value to point to a colum on the table indicating the time that the input text columns were last changed. schedule can be set to any cron-like value. Alternatively, schedule => 'realtime creates triggers on the source table and updates embeddings anytime new records are inserted to the source table or existing records are updated. Statements below would will result in new embeddings being generated either immediately ( schedule => 'realtime' ) or within the cron schedule set in the schedule parameter. INSERT INTO products ( product_id , product_name , description , product_category , price ) VALUES ( 12345 , 'pizza' , 'dish of Italian origin consisting of a flattened disk of bread' , 'food' , 5 . 99 ); UPDATE products SET description = 'sling made of fabric, rope, or netting, suspended between two or more points, used for swinging, sleeping, or resting' WHERE product_name = 'Hammock' ; Directly Interact with LLMs \u00b6 Sometimes you want more control over the handling of embeddings. For those situations you can directly call various LLM providers using SQL: For text generation: select vectorize . generate ( input => 'Tell me the difference between a cat and a dog in 1 sentence' , model => 'openai/gpt-4o' ); generate ----------------------------------------------------------------------------------------------------------- Cats are generally more independent and solitary, while dogs tend to be more social and loyal companions. (1 row) And for embedding generation: select vectorize . encode ( input => 'Tell me the difference between a cat and a dog in 1 sentence' , model => 'openai/text-embedding-3-large' ); {0.0028769304,-0.005826319,-0.0035932811, ...} Importing Pre-existing Embeddings \u00b6 If you have already computed embeddings using a compatible model (e.g., using Sentence-Transformers directly), you can import these into pg_vectorize without recomputation: -- First create the vectorize project SELECT vectorize . table ( job_name => 'my_search' , relation => 'my_table' , primary_key => 'id' , columns => ARRAY [ 'content' ], transformer => 'sentence-transformers/all-MiniLM-L6-v2' ); -- Then import your pre-computed embeddings SELECT vectorize . import_embeddings ( job_name => 'my_search' , src_table => 'my_embeddings_table' , src_primary_key => 'id' , src_embeddings_col => 'embedding' ); The embeddings must match the dimensions of the specified transformer model. For example, 'sentence-transformers/all-MiniLM-L6-v2' expects 384-dimensional vectors. Creating a Table from Existing Embeddings \u00b6 If you have already computed embeddings using a compatible model, you can create a new vectorize table directly from them: -- Create a vectorize table from existing embeddings SELECT vectorize . table_from ( relation => 'my_table' , columns => ARRAY [ 'content' ], job_name => 'my_search' , primary_key => 'id' , src_table => 'my_embeddings_table' , src_primary_key => 'id' , src_embeddings_col => 'embedding' , transformer => 'sentence-transformers/all-MiniLM-L6-v2' ); The embeddings must match the dimensions of the specified transformer model. This approach ensures your pre-computed embeddings are properly imported before any automatic updates are enabled. Contributing \u00b6 We welcome contributions from the community! If you're interested in contributing to pg_vectorize , please check out our Contributing Guide . Your contributions help make this project better for everyone. Community Support \u00b6 If you encounter any issues or have any questions, feel free to join our Tembo Community Slack . We're here to help!","title":"Vectorize"},{"location":"#features","text":"Workflows for both vector search and RAG Integrations with OpenAI's embeddings and Text-Generation endpoints and a self-hosted container for running Hugging Face Sentence-Transformers Automated creation of Postgres triggers to keep your embeddings up to date High level API - one function to initialize embeddings transformations, and another function to search","title":"Features"},{"location":"#table-of-contents","text":"Features Table of Contents Installation Vector Search Example RAG Example Updating Embeddings Directly Interact with LLMs Importing Pre-existing Embeddings Creating a Table from Existing Embeddings","title":"Table of Contents"},{"location":"#installation","text":"The fastest way to get started is by running the Tembo docker container and the vector server with docker compose: docker compose up -d Then connect to Postgres: docker compose exec -it postgres psql Enable the extension and its dependencies CREATE EXTENSION vectorize CASCADE ; Install into an existing Postgres instance If you're installing in an existing Postgres instance, you will need the following dependencies: Rust: - [pgrx toolchain](https://github.com/pgcentralfoundation/pgrx) Postgres Extensions: - [pg_cron](https://github.com/citusdata/pg_cron) ^1.5 - [pgmq](https://github.com/tembo-io/pgmq) ^1 - [pgvector](https://github.com/pgvector/pgvector) ^0.5.0 Then set the following either in postgresql.conf or as a configuration parameter: -- requires restart of Postgres alter system set shared_preload_libraries = 'vectorize,pg_cron' ; alter system set cron . database_name = 'postgres' ; And if you're running the vector-serve container, set the following url as a configuration parameter in Postgres. The host may need to change from `localhost` to something else depending on where you are running the container. alter system set vectorize . embedding_service_url = 'http://localhost:3000/v1' ; SELECT pg_reload_conf ();","title":"Installation"},{"location":"#vector-search-example","text":"Text-to-embedding transformation can be done with either Hugging Face's Sentence-Transformers or OpenAI's embeddings. The following examples use Hugging Face's Sentence-Transformers. See the project documentation for OpenAI examples. Follow the installation steps if you haven't already. Setup a products table. Copy from the example data provided by the extension. CREATE TABLE products ( LIKE vectorize . example_products INCLUDING ALL ); INSERT INTO products SELECT * FROM vectorize . example_products ; SELECT * FROM products limit 2 ; product_id | product_name | description | last_updated_at ------------+--------------+--------------------------------------------------------+------------------------------- 1 | Pencil | Utensil used for writing and often works best on paper | 2023-07-26 17:20:43.639351-05 2 | Laptop Stand | Elevated platform for laptops, enhancing ergonomics | 2023-07-26 17:20:43.639351-05 Create a job to vectorize the products table. We'll specify the tables primary key (product_id) and the columns that we want to search (product_name and description). SELECT vectorize . table ( job_name => 'product_search_hf' , relation => 'products' , primary_key => 'product_id' , columns => ARRAY [ 'product_name' , 'description' ], transformer => 'sentence-transformers/all-MiniLM-L6-v2' , schedule => 'realtime' ); This adds a new column to your table, in our case it is named product_search_embeddings , then populates that data with the transformed embeddings from the product_name and description columns. Then search, SELECT * FROM vectorize . search ( job_name => 'product_search_hf' , query => 'accessories for mobile devices' , return_columns => ARRAY [ 'product_id' , 'product_name' ], num_results => 3 ); search_results --------------------------------------------------------------------------------------------- {\"product_id\": 13, \"product_name\": \"Phone Charger\", \"similarity_score\": 0.8147814132322894} {\"product_id\": 6, \"product_name\": \"Backpack\", \"similarity_score\": 0.7743061352550308} {\"product_id\": 11, \"product_name\": \"Stylus Pen\", \"similarity_score\": 0.7709902653575383}","title":"Vector Search Example"},{"location":"#rag-example","text":"Ask raw text questions of the example products dataset and get chat responses from an OpenAI LLM. Follow the installation steps if you haven't already. Set the OpenAI API key , this is required to for use with OpenAI's chat-completion models. ALTER SYSTEM SET vectorize . openai_key TO '<your api key>' ; SELECT pg_reload_conf (); Create an example table if it does not already exist. CREATE TABLE products ( LIKE vectorize . example_products INCLUDING ALL ); INSERT INTO products SELECT * FROM vectorize . example_products ; Initialize a table for RAG. We'll use an open source Sentence Transformer to generate embeddings. Create a new column that we want to use as the context. In this case, we'll concatenate both product_name and description . ALTER TABLE products ADD COLUMN context TEXT GENERATED ALWAYS AS ( product_name || ': ' || description ) STORED ; Initialize the RAG project. We'll use the openai/text-embedding-3-small model to generate embeddings on our source documents. SELECT vectorize . table ( job_name => 'product_chat' , relation => 'products' , primary_key => 'product_id' , columns => ARRAY [ 'context' ], transformer => 'openai/text-embedding-3-small' , schedule => 'realtime' ); Now we can ask questions of the products table and get responses from the product_chat agent using the openai/gpt-3.5-turbo generative model. SELECT vectorize . rag ( job_name => 'product_chat' , query => 'What is a pencil?' , chat_model => 'openai/gpt-3.5-turbo' ) -> 'chat_response' ; \"A pencil is an item that is commonly used for writing and is known to be most effective on paper.\" And to use a locally hosted Ollama service, change the chat_model parameter: SELECT vectorize . rag ( job_name => 'product_chat' , query => 'What is a pencil?' , chat_model => 'ollama/wizardlm2:7b' ) -> 'chat_response' ; \" A pencil is a writing instrument that consists of a solid or gelignola wood core, known as the \\\"lead,\\\" encased in a cylindrical piece of breakable material (traditionally wood or plastic), which serves as the body of the pencil. The tip of the body is tapered to a point for writing, and it can mark paper with the imprint of the lead. When used on a sheet of paper, the combination of the pencil's lead and the paper creates a visible mark that is distinct from unmarked areas of the paper. Pencils are particularly well-suited for writing on paper, as they allow for precise control over the marks made.\" :bulb: Note that the -> 'chat_response' addition selects for that field of the JSON object output. Removing it will show the full JSON object, including information on which documents were included in the contextual prompt.","title":"RAG Example"},{"location":"#updating-embeddings","text":"When the source text data is updated, how and when the embeddings are updated is determined by the value set to the schedule parameter in vectorize.table . The default behavior is schedule => '* * * * *' , which means the background worker process checks for changes every minute, and updates the embeddings accordingly. This method requires setting the updated_at_col value to point to a colum on the table indicating the time that the input text columns were last changed. schedule can be set to any cron-like value. Alternatively, schedule => 'realtime creates triggers on the source table and updates embeddings anytime new records are inserted to the source table or existing records are updated. Statements below would will result in new embeddings being generated either immediately ( schedule => 'realtime' ) or within the cron schedule set in the schedule parameter. INSERT INTO products ( product_id , product_name , description , product_category , price ) VALUES ( 12345 , 'pizza' , 'dish of Italian origin consisting of a flattened disk of bread' , 'food' , 5 . 99 ); UPDATE products SET description = 'sling made of fabric, rope, or netting, suspended between two or more points, used for swinging, sleeping, or resting' WHERE product_name = 'Hammock' ;","title":"Updating Embeddings"},{"location":"#directly-interact-with-llms","text":"Sometimes you want more control over the handling of embeddings. For those situations you can directly call various LLM providers using SQL: For text generation: select vectorize . generate ( input => 'Tell me the difference between a cat and a dog in 1 sentence' , model => 'openai/gpt-4o' ); generate ----------------------------------------------------------------------------------------------------------- Cats are generally more independent and solitary, while dogs tend to be more social and loyal companions. (1 row) And for embedding generation: select vectorize . encode ( input => 'Tell me the difference between a cat and a dog in 1 sentence' , model => 'openai/text-embedding-3-large' ); {0.0028769304,-0.005826319,-0.0035932811, ...}","title":"Directly Interact with LLMs"},{"location":"#importing-pre-existing-embeddings","text":"If you have already computed embeddings using a compatible model (e.g., using Sentence-Transformers directly), you can import these into pg_vectorize without recomputation: -- First create the vectorize project SELECT vectorize . table ( job_name => 'my_search' , relation => 'my_table' , primary_key => 'id' , columns => ARRAY [ 'content' ], transformer => 'sentence-transformers/all-MiniLM-L6-v2' ); -- Then import your pre-computed embeddings SELECT vectorize . import_embeddings ( job_name => 'my_search' , src_table => 'my_embeddings_table' , src_primary_key => 'id' , src_embeddings_col => 'embedding' ); The embeddings must match the dimensions of the specified transformer model. For example, 'sentence-transformers/all-MiniLM-L6-v2' expects 384-dimensional vectors.","title":"Importing Pre-existing Embeddings"},{"location":"#creating-a-table-from-existing-embeddings","text":"If you have already computed embeddings using a compatible model, you can create a new vectorize table directly from them: -- Create a vectorize table from existing embeddings SELECT vectorize . table_from ( relation => 'my_table' , columns => ARRAY [ 'content' ], job_name => 'my_search' , primary_key => 'id' , src_table => 'my_embeddings_table' , src_primary_key => 'id' , src_embeddings_col => 'embedding' , transformer => 'sentence-transformers/all-MiniLM-L6-v2' ); The embeddings must match the dimensions of the specified transformer model. This approach ensures your pre-computed embeddings are properly imported before any automatic updates are enabled.","title":"Creating a Table from Existing Embeddings"},{"location":"#contributing","text":"We welcome contributions from the community! If you're interested in contributing to pg_vectorize , please check out our Contributing Guide . Your contributions help make this project better for everyone.","title":"Contributing"},{"location":"#community-support","text":"If you encounter any issues or have any questions, feel free to join our Tembo Community Slack . We're here to help!","title":"Community Support"},{"location":"configuration/","text":"Configuring pg_vectorize \u00b6 Changing the database \u00b6 To change the database that pg_vectorize background worker is connected to, you can use the following SQL command: ALTER SYSTEM SET vectorize . database_name TO 'my_new_db' ; Then, restart Postgres. Changing Embedding and LLM base URLs \u00b6 All Embedding model and LLM providers can have their base URLs changed. For example, if you have an OpenAI compliant embedding or LLM server (such as vLLM ), running at https://api.myserver.com/v1 , you can change the base URL with the following SQL command: ALTER SYSTEM SET vectorize . openai_service_url TO 'https://api.myserver.com/v1' ; SELECT pg_reload_conf (); Changing the batch job size \u00b6 Text data stored in Postgres is transformed into embeddings via HTTP requests made from the pg_vectorize background worker. Requests are made to the specified embedding service in batch (multiple inputs per request). The number of inputs per request is determined by the vectorize.batch_size GUC. This has no impact on transformations that occur during vectorize.search() , vectorize.encode() and vectorize.rag() which are always batch size 1 since those APIs accept only a single input (the raw text query). ALTER SYSTEM SET vectorize . batch_size to 100 ; Available GUCs \u00b6 The complete list of GUCs available for pg_vectorize are defined in extension/src/guc.rs .","title":"Extension Configuration"},{"location":"configuration/#configuring-pg_vectorize","text":"","title":"Configuring pg_vectorize"},{"location":"configuration/#changing-the-database","text":"To change the database that pg_vectorize background worker is connected to, you can use the following SQL command: ALTER SYSTEM SET vectorize . database_name TO 'my_new_db' ; Then, restart Postgres.","title":"Changing the database"},{"location":"configuration/#changing-embedding-and-llm-base-urls","text":"All Embedding model and LLM providers can have their base URLs changed. For example, if you have an OpenAI compliant embedding or LLM server (such as vLLM ), running at https://api.myserver.com/v1 , you can change the base URL with the following SQL command: ALTER SYSTEM SET vectorize . openai_service_url TO 'https://api.myserver.com/v1' ; SELECT pg_reload_conf ();","title":"Changing Embedding and LLM base URLs"},{"location":"configuration/#changing-the-batch-job-size","text":"Text data stored in Postgres is transformed into embeddings via HTTP requests made from the pg_vectorize background worker. Requests are made to the specified embedding service in batch (multiple inputs per request). The number of inputs per request is determined by the vectorize.batch_size GUC. This has no impact on transformations that occur during vectorize.search() , vectorize.encode() and vectorize.rag() which are always batch size 1 since those APIs accept only a single input (the raw text query). ALTER SYSTEM SET vectorize . batch_size to 100 ;","title":"Changing the batch job size"},{"location":"configuration/#available-gucs","text":"The complete list of GUCs available for pg_vectorize are defined in extension/src/guc.rs .","title":"Available GUCs"},{"location":"api/","text":"PG Vectorize API Overview \u00b6 pg vectorize provides tools for two closely related tasks; vector search and retrieval augmented generation (RAG), and there are APIs dedicated to both of these tasks. Vector search is an important component of RAG and the RAG APIs depend on the vector search APIs. It could be helpful to think of the vector search APIs as lower level than RAG. However, relative to Postgres's APIs, both of these vectorize APIs are very high level. Importing Pre-existing Embeddings \u00b6 If you have already computed embeddings for your data using a compatible model, you can import these directly into pg_vectorize using the vectorize.import_embeddings function: SELECT vectorize . import_embeddings ( job_name => 'my_search_project' , src_table => 'my_source_table' , src_primary_key => 'id' , src_embeddings_col => 'embeddings' ); This function allows you to: - Import pre-computed embeddings without recomputation - Support both join and append table methods - Automatically validate embedding dimensions - Clean up any pending realtime jobs The embeddings must match the dimensions expected by the model specified when creating the project with vectorize.table() . Parameters \u00b6 job_name : The name of your pg_vectorize project (created via vectorize.table() ) src_table : The table containing your pre-computed embeddings src_primary_key : The primary key column in your source table src_embeddings_col : The column containing the vector embeddings Example \u00b6 -- First create a vectorize project SELECT vectorize . table ( job_name => 'product_search' , relation => 'products' , primary_key => 'id' , columns => ARRAY [ 'description' ], transformer => 'sentence-transformers/all-MiniLM-L6-v2' ); -- Then import pre-existing embeddings SELECT vectorize . import_embeddings ( job_name => 'product_search' , src_table => 'product_embeddings' , src_primary_key => 'product_id' , src_embeddings_col => 'embedding_vector' ); Creating a Table from Existing Embeddings \u00b6 If you have pre-computed embeddings and want to create a new vectorize table from them, use vectorize.table_from() : SELECT vectorize . table_from ( relation => 'products' , columns => ARRAY [ 'description' ], job_name => 'product_search' , primary_key => 'id' , src_table => 'product_embeddings' , src_primary_key => 'product_id' , src_embeddings_col => 'embedding_vector' , transformer => 'sentence-transformers/all-MiniLM-L6-v2' , schedule => 'realtime' ); This function: 1. Creates the vectorize table structure 2. Imports your pre-computed embeddings 3. Sets up the specified update schedule (realtime triggers or cron job) The embeddings must match the dimensions of the specified transformer model. Parameters \u00b6 relation : The table to create or modify columns : Array of columns to generate embeddings from job_name : Name for this vectorize project primary_key : Primary key column in your table src_table : Table containing your pre-computed embeddings src_primary_key : Primary key column in your source table src_embeddings_col : Column containing the vector embeddings schema : Schema name (default: 'public') update_col : Column tracking updates (default: 'last_updated_at') index_dist_type : Index type (default: 'pgv_hnsw_cosine') transformer : Model to use (default: 'sentence-transformers/all-MiniLM-L6-v2') table_method : How to store embeddings (default: 'join') schedule : Update schedule - 'realtime', 'manual', or cron expression (default: ' * * * ') This approach ensures your pre-computed embeddings are properly imported before any automatic updates are enabled.","title":"Overview"},{"location":"api/#pg-vectorize-api-overview","text":"pg vectorize provides tools for two closely related tasks; vector search and retrieval augmented generation (RAG), and there are APIs dedicated to both of these tasks. Vector search is an important component of RAG and the RAG APIs depend on the vector search APIs. It could be helpful to think of the vector search APIs as lower level than RAG. However, relative to Postgres's APIs, both of these vectorize APIs are very high level.","title":"PG Vectorize API Overview"},{"location":"api/#importing-pre-existing-embeddings","text":"If you have already computed embeddings for your data using a compatible model, you can import these directly into pg_vectorize using the vectorize.import_embeddings function: SELECT vectorize . import_embeddings ( job_name => 'my_search_project' , src_table => 'my_source_table' , src_primary_key => 'id' , src_embeddings_col => 'embeddings' ); This function allows you to: - Import pre-computed embeddings without recomputation - Support both join and append table methods - Automatically validate embedding dimensions - Clean up any pending realtime jobs The embeddings must match the dimensions expected by the model specified when creating the project with vectorize.table() .","title":"Importing Pre-existing Embeddings"},{"location":"api/#parameters","text":"job_name : The name of your pg_vectorize project (created via vectorize.table() ) src_table : The table containing your pre-computed embeddings src_primary_key : The primary key column in your source table src_embeddings_col : The column containing the vector embeddings","title":"Parameters"},{"location":"api/#example","text":"-- First create a vectorize project SELECT vectorize . table ( job_name => 'product_search' , relation => 'products' , primary_key => 'id' , columns => ARRAY [ 'description' ], transformer => 'sentence-transformers/all-MiniLM-L6-v2' ); -- Then import pre-existing embeddings SELECT vectorize . import_embeddings ( job_name => 'product_search' , src_table => 'product_embeddings' , src_primary_key => 'product_id' , src_embeddings_col => 'embedding_vector' );","title":"Example"},{"location":"api/#creating-a-table-from-existing-embeddings","text":"If you have pre-computed embeddings and want to create a new vectorize table from them, use vectorize.table_from() : SELECT vectorize . table_from ( relation => 'products' , columns => ARRAY [ 'description' ], job_name => 'product_search' , primary_key => 'id' , src_table => 'product_embeddings' , src_primary_key => 'product_id' , src_embeddings_col => 'embedding_vector' , transformer => 'sentence-transformers/all-MiniLM-L6-v2' , schedule => 'realtime' ); This function: 1. Creates the vectorize table structure 2. Imports your pre-computed embeddings 3. Sets up the specified update schedule (realtime triggers or cron job) The embeddings must match the dimensions of the specified transformer model.","title":"Creating a Table from Existing Embeddings"},{"location":"api/#parameters_1","text":"relation : The table to create or modify columns : Array of columns to generate embeddings from job_name : Name for this vectorize project primary_key : Primary key column in your table src_table : Table containing your pre-computed embeddings src_primary_key : Primary key column in your source table src_embeddings_col : Column containing the vector embeddings schema : Schema name (default: 'public') update_col : Column tracking updates (default: 'last_updated_at') index_dist_type : Index type (default: 'pgv_hnsw_cosine') transformer : Model to use (default: 'sentence-transformers/all-MiniLM-L6-v2') table_method : How to store embeddings (default: 'join') schedule : Update schedule - 'realtime', 'manual', or cron expression (default: ' * * * ') This approach ensures your pre-computed embeddings are properly imported before any automatic updates are enabled.","title":"Parameters"},{"location":"api/rag/","text":"RAG \u00b6 SQL API for Retrieval Augmented Generation projects. Initializing a RAG table \u00b6 Creates embeddings for specified data in a Postgres table. Creates index, and triggers to keep embeddings up to date. vectorize.table \u00b6 vectorize . \"table\" ( \"relation\" TEXT , \"columns\" TEXT [], \"job_name\" TEXT , \"primary_key\" TEXT , \"schema\" TEXT DEFAULT 'public' , \"update_col\" TEXT DEFAULT 'last_updated_at' , \"transformer\" TEXT DEFAULT 'sentence-transformers/all-MiniLM-L6-v2' , \"index_dist_type\" vectorize . IndexDist DEFAULT 'pgv_hnsw_cosine' , \"table_method\" vectorize . TableMethod DEFAULT 'join' , \"schedule\" TEXT DEFAULT '* * * * *' ) RETURNS TEXT Parameter Type Description relation text The name of the table to be initialized. columns text The name of the columns that contains the content that is used for context for RAG. Multiple columns are concatenated. job_name text A unique name for the project. primary_key text The name of the column that contains the unique record id. args json Additional arguments for the transformer. Defaults to '{}'. schema text The name of the schema where the table is located. Defaults to 'public'. update_col text Column specifying the last time the record was updated. Required for cron-like schedule. Defaults to last_updated_at transformer text The name of the transformer to use for the embeddings. Defaults to 'text-embedding-ada-002'. index_dist_type IndexDist The name of index type to build. Defaults to 'pgv_hnsw_cosine'. table_method TableMethod join to store embeddings in a new table in the vectorize schema. append to create columns for embeddings on the source table. Defaults to join . schedule text Accepts a cron-like input for a cron based updates. Or realtime to set up a trigger. Example: select vectorize . table ( job_name => 'tembo_chat' , \"table\" => 'tembo_docs' , primary_key => 'document_name' , columns => ARRAY [ 'content' ], transformer => 'sentence-transformers/all-MiniLM-L12-v2' ); Query using RAG \u00b6 vectorize.rag \u00b6 vectorize . \"rag\" ( \"agent_name\" TEXT , \"query\" TEXT , \"chat_model\" TEXT DEFAULT 'openai/gpt-3.5-turbo' , \"task\" TEXT DEFAULT 'question_answer' , \"api_key\" TEXT DEFAULT NULL , \"num_context\" INT DEFAULT 2 , \"force_trim\" bool DEFAULT false ) RETURNS TABLE ( \"chat_results\" jsonb ) Parameters: Parameter Type Description job_name text Specify the name provided during vectorize.table query text The user provided query or command provided to the chat completion model. task text Specifies the name of the prompt template to use. Must exist in vectorize.prompts (prompt_type) api_key text API key for the specified chat model. If OpenAI, this value overrides the config vectorize.openai_key num_context int The number of context documents returned by similarity search include in the message submitted to the chat completion model force_trim bool Trims the documents provided as context, starting with the least relevant documents, such that the prompt fits into the model's context window. Defaults to false. Example \u00b6 select vectorize . rag ( job_name => 'tembo_support' , query => 'what are the major features from the tembo kubernetes operator?' , chat_model => 'openai/gpt-3.5-turbo' , force_trim => 'true' ); The response contains the contextual data used in the prompt in addition to the chat response. { \"context\" : [ { \"content\" : \"\\\"Tembo Standard Stack\\\\n\\\\nThe Tembo Standard Stack is a tuned Postgres instance balance for general purpose computing. You have full control over compute, configuration, and extension installation.\\\"\" , \"token_ct\" : 37 , \"record_id\" : \"535\" }, { \"content\" : \"\\\"Why Stacks?\\\\n\\\\nAdopting a new database adds significant complexity and costs to an engineering organization. Organizations spend a huge amount of time evaluating, benchmarking or migrating databases and setting upcomplicated pipelines keeping those databases in sync.\\\\n\\\\nMost of these use cases can be served by Postgres, thanks to its stability, feature completeness and extensibility. However, optimizing Postgres for each use case is a non-trivial task and requires domain expertise, use case understanding and deep Postgres expertise, making it hard for most developers to adopt this.\\\\n\\\\nTembo Stacks solve that problem by providing pre-built, use case optimized Postgres deployments.\\\\n\\\\nA tembo stack is a pre-built, use case specific Postgres deployment which enables you to quickly deploy specialized data services that can replace external, non-Postgres data services. They help you avoid the pains associated with adopting, operationalizing, optimizing and managing new databases.\\\\n\\\\n|Name|Replacement for|\\\\n|----|---------------|\\\\n|Data Warehouse| Snowflake, Bigquery |\\\\n|Geospatial| ESRI, Oracle |\\\\n|OLTP| Amazon RDS |\\\\n|OLAP| Snowflake, Bigquery |\\\\n|Machine Learning| MindsDB |\\\\n|Message Queue| Amazon SQS, RabbitMQ, Redis |\\\\n|Mongo Alternative on Postgres| MongoDB |\\\\n|RAG| LangChain |\\\\n|Standard| Amazon RDS |\\\\n|Vector DB| Pinecone, Weaviate |\\\\n\\\\nWe are actively working on additional Stacks. Check out the Tembo Roadmap and upvote the stacks you''d like to see next.\\\"\" , \"token_ct\" : 336 , \"record_id\" : \"387\" } ], \"chat_response\" : \"Tembo Stacks are pre-built, use case specific Postgres deployments that are optimized for various data services such as Data Warehouse, Geospatial, OLTP, OLAP, Machine Learning, Message Queue, and more. These Stacks aim to provide organizations with specialized data services that can replace external non-Postgres data services. Each Tembo Stack is designed to cater to specific use cases, enabling developers to quickly deploy and utilize Postgres instances tailored to their needs without the complexity of setting up and optimizing Postgres manually.\" } Filter the results to just the chat_response : select vectorize . rag ( job_name => 'tembo_support' , query => 'what are the major features from the tembo kubernetes operator?' , chat_model => 'gpt-3.5-turbo' , force_trim => 'true' ) -> 'chat_response' ; \"Tembo Stacks are pre-built, use case specific Postgres deployments that are optimized for various data services such as Data Warehouse, Geospatial, OLTP, OLAP, Machine Learning, Message Queue, and more. These Stacks aim to provide organizations with specialized data services that can replace external non-Postgres data services. Each Tembo Stack is designed to cater to specific use cases, enabling developers to quickly deploy and utilize Postgres instances tailored to their needs without the complexity of setting up and optimizing Postgres manually.\"","title":"RAG"},{"location":"api/rag/#rag","text":"SQL API for Retrieval Augmented Generation projects.","title":"RAG"},{"location":"api/rag/#initializing-a-rag-table","text":"Creates embeddings for specified data in a Postgres table. Creates index, and triggers to keep embeddings up to date.","title":"Initializing a RAG table"},{"location":"api/rag/#vectorizetable","text":"vectorize . \"table\" ( \"relation\" TEXT , \"columns\" TEXT [], \"job_name\" TEXT , \"primary_key\" TEXT , \"schema\" TEXT DEFAULT 'public' , \"update_col\" TEXT DEFAULT 'last_updated_at' , \"transformer\" TEXT DEFAULT 'sentence-transformers/all-MiniLM-L6-v2' , \"index_dist_type\" vectorize . IndexDist DEFAULT 'pgv_hnsw_cosine' , \"table_method\" vectorize . TableMethod DEFAULT 'join' , \"schedule\" TEXT DEFAULT '* * * * *' ) RETURNS TEXT Parameter Type Description relation text The name of the table to be initialized. columns text The name of the columns that contains the content that is used for context for RAG. Multiple columns are concatenated. job_name text A unique name for the project. primary_key text The name of the column that contains the unique record id. args json Additional arguments for the transformer. Defaults to '{}'. schema text The name of the schema where the table is located. Defaults to 'public'. update_col text Column specifying the last time the record was updated. Required for cron-like schedule. Defaults to last_updated_at transformer text The name of the transformer to use for the embeddings. Defaults to 'text-embedding-ada-002'. index_dist_type IndexDist The name of index type to build. Defaults to 'pgv_hnsw_cosine'. table_method TableMethod join to store embeddings in a new table in the vectorize schema. append to create columns for embeddings on the source table. Defaults to join . schedule text Accepts a cron-like input for a cron based updates. Or realtime to set up a trigger. Example: select vectorize . table ( job_name => 'tembo_chat' , \"table\" => 'tembo_docs' , primary_key => 'document_name' , columns => ARRAY [ 'content' ], transformer => 'sentence-transformers/all-MiniLM-L12-v2' );","title":"vectorize.table"},{"location":"api/rag/#query-using-rag","text":"","title":"Query using RAG"},{"location":"api/rag/#vectorizerag","text":"vectorize . \"rag\" ( \"agent_name\" TEXT , \"query\" TEXT , \"chat_model\" TEXT DEFAULT 'openai/gpt-3.5-turbo' , \"task\" TEXT DEFAULT 'question_answer' , \"api_key\" TEXT DEFAULT NULL , \"num_context\" INT DEFAULT 2 , \"force_trim\" bool DEFAULT false ) RETURNS TABLE ( \"chat_results\" jsonb ) Parameters: Parameter Type Description job_name text Specify the name provided during vectorize.table query text The user provided query or command provided to the chat completion model. task text Specifies the name of the prompt template to use. Must exist in vectorize.prompts (prompt_type) api_key text API key for the specified chat model. If OpenAI, this value overrides the config vectorize.openai_key num_context int The number of context documents returned by similarity search include in the message submitted to the chat completion model force_trim bool Trims the documents provided as context, starting with the least relevant documents, such that the prompt fits into the model's context window. Defaults to false.","title":"vectorize.rag"},{"location":"api/rag/#example","text":"select vectorize . rag ( job_name => 'tembo_support' , query => 'what are the major features from the tembo kubernetes operator?' , chat_model => 'openai/gpt-3.5-turbo' , force_trim => 'true' ); The response contains the contextual data used in the prompt in addition to the chat response. { \"context\" : [ { \"content\" : \"\\\"Tembo Standard Stack\\\\n\\\\nThe Tembo Standard Stack is a tuned Postgres instance balance for general purpose computing. You have full control over compute, configuration, and extension installation.\\\"\" , \"token_ct\" : 37 , \"record_id\" : \"535\" }, { \"content\" : \"\\\"Why Stacks?\\\\n\\\\nAdopting a new database adds significant complexity and costs to an engineering organization. Organizations spend a huge amount of time evaluating, benchmarking or migrating databases and setting upcomplicated pipelines keeping those databases in sync.\\\\n\\\\nMost of these use cases can be served by Postgres, thanks to its stability, feature completeness and extensibility. However, optimizing Postgres for each use case is a non-trivial task and requires domain expertise, use case understanding and deep Postgres expertise, making it hard for most developers to adopt this.\\\\n\\\\nTembo Stacks solve that problem by providing pre-built, use case optimized Postgres deployments.\\\\n\\\\nA tembo stack is a pre-built, use case specific Postgres deployment which enables you to quickly deploy specialized data services that can replace external, non-Postgres data services. They help you avoid the pains associated with adopting, operationalizing, optimizing and managing new databases.\\\\n\\\\n|Name|Replacement for|\\\\n|----|---------------|\\\\n|Data Warehouse| Snowflake, Bigquery |\\\\n|Geospatial| ESRI, Oracle |\\\\n|OLTP| Amazon RDS |\\\\n|OLAP| Snowflake, Bigquery |\\\\n|Machine Learning| MindsDB |\\\\n|Message Queue| Amazon SQS, RabbitMQ, Redis |\\\\n|Mongo Alternative on Postgres| MongoDB |\\\\n|RAG| LangChain |\\\\n|Standard| Amazon RDS |\\\\n|Vector DB| Pinecone, Weaviate |\\\\n\\\\nWe are actively working on additional Stacks. Check out the Tembo Roadmap and upvote the stacks you''d like to see next.\\\"\" , \"token_ct\" : 336 , \"record_id\" : \"387\" } ], \"chat_response\" : \"Tembo Stacks are pre-built, use case specific Postgres deployments that are optimized for various data services such as Data Warehouse, Geospatial, OLTP, OLAP, Machine Learning, Message Queue, and more. These Stacks aim to provide organizations with specialized data services that can replace external non-Postgres data services. Each Tembo Stack is designed to cater to specific use cases, enabling developers to quickly deploy and utilize Postgres instances tailored to their needs without the complexity of setting up and optimizing Postgres manually.\" } Filter the results to just the chat_response : select vectorize . rag ( job_name => 'tembo_support' , query => 'what are the major features from the tembo kubernetes operator?' , chat_model => 'gpt-3.5-turbo' , force_trim => 'true' ) -> 'chat_response' ; \"Tembo Stacks are pre-built, use case specific Postgres deployments that are optimized for various data services such as Data Warehouse, Geospatial, OLTP, OLAP, Machine Learning, Message Queue, and more. These Stacks aim to provide organizations with specialized data services that can replace external non-Postgres data services. Each Tembo Stack is designed to cater to specific use cases, enabling developers to quickly deploy and utilize Postgres instances tailored to their needs without the complexity of setting up and optimizing Postgres manually.\"","title":"Example"},{"location":"api/search/","text":"Vector Search \u00b6 The vector-search flow is two part; first initialize a table using vectorize.table() , then search the table with vectorize.search() . Initialize a table \u00b6 Initialize a table for vector search. Generates embeddings and index. Creates triggers to keep embeddings up-to-date. vectorize . \"table\" ( \"relation\" TEXT , \"columns\" TEXT [], \"job_name\" TEXT , \"primary_key\" TEXT , \"schema\" TEXT DEFAULT 'public' , \"update_col\" TEXT DEFAULT 'last_updated_at' , \"transformer\" TEXT DEFAULT 'sentence-transformers/all-MiniLM-L6-v2' , \"index_dist_type\" vectorize . IndexDist DEFAULT 'pgv_hnsw_cosine' , \"table_method\" vectorize . TableMethod DEFAULT 'join' , \"schedule\" TEXT DEFAULT '* * * * *' ) RETURNS TEXT Parameter Type Description relation text The name of the table to be initialized. columns text The name of the columns that contains the content that is used for context for RAG. Multiple columns are concatenated. job_name text A unique name for the project. primary_key text The name of the column that contains the unique record id. args json Additional arguments for the transformer. Defaults to '{}'. schema text The name of the schema where the table is located. Defaults to 'public'. update_col text Column specifying the last time the record was updated. Required for cron-like schedule. Defaults to last_updated_at transformer text The name of the transformer to use for the embeddings. Defaults to 'text-embedding-ada-002'. index_dist_type IndexDist The name of index type to build. Defaults to 'pgv_hnsw_cosine'. table_method TableMethod join to store embeddings in a new table in the vectorize schema. append to create columns for embeddings on the source table. Defaults to join . schedule text Accepts a cron-like input for a cron based updates. Or realtime to set up a trigger. Sentence-Transformer Examples \u00b6 OpenAI Examples \u00b6 To use embedding model provided by OpenAI's public embedding endpoints, provide the model name into the transformer parameter, and provide the OpenAI API key. Pass the API key into the function call via args . select vectorize . table ( job_name => 'product_search' , relation => 'products' , primary_key => 'product_id' , columns => ARRAY [ 'product_name' , 'description' ], transformer => 'openai/text-embedding-ada-002' , args => '{\"api_key\": \"my-openai-key\"}' ); The API key can also be set via GUC. ALTER SYSTEM SET vectorize . openai_key TO 'my-openai-key' ; SELECT pg_reload_conf (); Then call vectorize.table() without providing the API key. select vectorize . table ( job_name => 'product_search' , relation => 'products' , primary_key => 'product_id' , columns => ARRAY [ 'product_name' , 'description' ], transformer => 'openai/text-embedding-ada-002' ); Search a table \u00b6 Search a table initialized with vectorize.table . The search results are sorted in descending order according to similarity. The query is transformed to embeddings using the same transformer configured during vectorize.table . The where_sql parameter is used to apply additional filtering to the search results based on SQL conditions. vectorize . \"search\" ( \"job_name\" TEXT , \"query\" TEXT , \"api_key\" TEXT DEFAULT NULL , \"return_columns\" TEXT [] DEFAULT ARRAY [ '*' ]:: text [], \"num_results\" INT DEFAULT 10 \"where_sql\" TEXT DEFAULT NULL ) RETURNS TABLE ( \"search_results\" jsonb ) Parameters: Parameter Type Description job_name text A unique name for the project. query text The user provided query or command provided to the chat completion model. api_key text API key for the specified chat model. If OpenAI, this value overrides the config vectorize.openai_key return_columns text[] The columns to return in the search results. Defaults to all columns. num_results int The number of results to return. Sorted in descending order according to similarity. Defaults to 10. where_sql text An optional SQL condition to filter the search results. This condition is applied after the similarity search. Example \u00b6 SELECT * FROM vectorize . search ( job_name => 'product_search' , query => 'mobile electronic devices' , return_columns => ARRAY [ 'product_id' , 'product_name' ], num_results => 3 ); search_results -------------------------------------------------------------------------------------------- ---- {\"product_id\": 13, \"product_name\": \"Phone Charger\", \"similarity_score\": 0.8564681325237845} {\"product_id\": 24, \"product_name\": \"Tablet Holder\", \"similarity_score\": 0.8295988934993099} {\"product_id\": 4, \"product_name\": \"Bluetooth Speaker\", \"similarity_score\": 0.8250355616233103} (3 rows) Filtering Search Results \u00b6 The where_sql parameter allows to apply SQL-based filtering after performing the vector similarity search. This feature is useful when you want to narrow down the search results based on certain conditions such as product category or price . Example \u00b6 SELECT * FROM vectorize . search ( job_name => 'product_search' , query => 'mobile electronic devices' , return_columns => ARRAY [ 'product_id' , 'product_name' ], num_results => 3 , where_sql => 'product_category = ''electronics'' AND price > 100' ); In the above example, the results are filtered where the product_category is electronics and the price is greater than 100. Optimizing Searches with Partial Indices \u00b6 For improving performance when using filters, you can create partial indices. This will speed up the execution of queries with frequent conditions in the where_sql parameter. Example \u00b6 CREATE INDEX idx_product_price ON products ( product_name ) WHERE price > 100 ; This index optimizes queries that search for products where the price is greater than 100. Note: Partial indices improve performance by only indexing rows that meet the specified condition. This reduces the amount of data the database needs to scan, making queries with the same filter more efficient since only relevant rows are included in the index. By combining the where_sql filtering feature with partial indices, you can efficiently narrow down search results and improve query performance.","title":"Vector Search"},{"location":"api/search/#vector-search","text":"The vector-search flow is two part; first initialize a table using vectorize.table() , then search the table with vectorize.search() .","title":"Vector Search"},{"location":"api/search/#initialize-a-table","text":"Initialize a table for vector search. Generates embeddings and index. Creates triggers to keep embeddings up-to-date. vectorize . \"table\" ( \"relation\" TEXT , \"columns\" TEXT [], \"job_name\" TEXT , \"primary_key\" TEXT , \"schema\" TEXT DEFAULT 'public' , \"update_col\" TEXT DEFAULT 'last_updated_at' , \"transformer\" TEXT DEFAULT 'sentence-transformers/all-MiniLM-L6-v2' , \"index_dist_type\" vectorize . IndexDist DEFAULT 'pgv_hnsw_cosine' , \"table_method\" vectorize . TableMethod DEFAULT 'join' , \"schedule\" TEXT DEFAULT '* * * * *' ) RETURNS TEXT Parameter Type Description relation text The name of the table to be initialized. columns text The name of the columns that contains the content that is used for context for RAG. Multiple columns are concatenated. job_name text A unique name for the project. primary_key text The name of the column that contains the unique record id. args json Additional arguments for the transformer. Defaults to '{}'. schema text The name of the schema where the table is located. Defaults to 'public'. update_col text Column specifying the last time the record was updated. Required for cron-like schedule. Defaults to last_updated_at transformer text The name of the transformer to use for the embeddings. Defaults to 'text-embedding-ada-002'. index_dist_type IndexDist The name of index type to build. Defaults to 'pgv_hnsw_cosine'. table_method TableMethod join to store embeddings in a new table in the vectorize schema. append to create columns for embeddings on the source table. Defaults to join . schedule text Accepts a cron-like input for a cron based updates. Or realtime to set up a trigger.","title":"Initialize a table"},{"location":"api/search/#sentence-transformer-examples","text":"","title":"Sentence-Transformer Examples"},{"location":"api/search/#openai-examples","text":"To use embedding model provided by OpenAI's public embedding endpoints, provide the model name into the transformer parameter, and provide the OpenAI API key. Pass the API key into the function call via args . select vectorize . table ( job_name => 'product_search' , relation => 'products' , primary_key => 'product_id' , columns => ARRAY [ 'product_name' , 'description' ], transformer => 'openai/text-embedding-ada-002' , args => '{\"api_key\": \"my-openai-key\"}' ); The API key can also be set via GUC. ALTER SYSTEM SET vectorize . openai_key TO 'my-openai-key' ; SELECT pg_reload_conf (); Then call vectorize.table() without providing the API key. select vectorize . table ( job_name => 'product_search' , relation => 'products' , primary_key => 'product_id' , columns => ARRAY [ 'product_name' , 'description' ], transformer => 'openai/text-embedding-ada-002' );","title":"OpenAI Examples"},{"location":"api/search/#search-a-table","text":"Search a table initialized with vectorize.table . The search results are sorted in descending order according to similarity. The query is transformed to embeddings using the same transformer configured during vectorize.table . The where_sql parameter is used to apply additional filtering to the search results based on SQL conditions. vectorize . \"search\" ( \"job_name\" TEXT , \"query\" TEXT , \"api_key\" TEXT DEFAULT NULL , \"return_columns\" TEXT [] DEFAULT ARRAY [ '*' ]:: text [], \"num_results\" INT DEFAULT 10 \"where_sql\" TEXT DEFAULT NULL ) RETURNS TABLE ( \"search_results\" jsonb ) Parameters: Parameter Type Description job_name text A unique name for the project. query text The user provided query or command provided to the chat completion model. api_key text API key for the specified chat model. If OpenAI, this value overrides the config vectorize.openai_key return_columns text[] The columns to return in the search results. Defaults to all columns. num_results int The number of results to return. Sorted in descending order according to similarity. Defaults to 10. where_sql text An optional SQL condition to filter the search results. This condition is applied after the similarity search.","title":"Search a table"},{"location":"api/search/#example","text":"SELECT * FROM vectorize . search ( job_name => 'product_search' , query => 'mobile electronic devices' , return_columns => ARRAY [ 'product_id' , 'product_name' ], num_results => 3 ); search_results -------------------------------------------------------------------------------------------- ---- {\"product_id\": 13, \"product_name\": \"Phone Charger\", \"similarity_score\": 0.8564681325237845} {\"product_id\": 24, \"product_name\": \"Tablet Holder\", \"similarity_score\": 0.8295988934993099} {\"product_id\": 4, \"product_name\": \"Bluetooth Speaker\", \"similarity_score\": 0.8250355616233103} (3 rows)","title":"Example"},{"location":"api/search/#filtering-search-results","text":"The where_sql parameter allows to apply SQL-based filtering after performing the vector similarity search. This feature is useful when you want to narrow down the search results based on certain conditions such as product category or price .","title":"Filtering Search Results"},{"location":"api/search/#example_1","text":"SELECT * FROM vectorize . search ( job_name => 'product_search' , query => 'mobile electronic devices' , return_columns => ARRAY [ 'product_id' , 'product_name' ], num_results => 3 , where_sql => 'product_category = ''electronics'' AND price > 100' ); In the above example, the results are filtered where the product_category is electronics and the price is greater than 100.","title":"Example"},{"location":"api/search/#optimizing-searches-with-partial-indices","text":"For improving performance when using filters, you can create partial indices. This will speed up the execution of queries with frequent conditions in the where_sql parameter.","title":"Optimizing Searches with Partial Indices"},{"location":"api/search/#example_2","text":"CREATE INDEX idx_product_price ON products ( product_name ) WHERE price > 100 ; This index optimizes queries that search for products where the price is greater than 100. Note: Partial indices improve performance by only indexing rows that meet the specified condition. This reduces the amount of data the database needs to scan, making queries with the same filter more efficient since only relevant rows are included in the index. By combining the where_sql filtering feature with partial indices, you can efficiently narrow down search results and improve query performance.","title":"Example"},{"location":"api/utilities/","text":"Utilities \u00b6 Text to Embeddings \u00b6 Transforms a block of text to embeddings using the specified transformer. Requires the vector-serve container to be set via vectorize.embedding_service_url , or an OpenAI key to be set if using OpenAI embedding models. vectorize . \"encode\" ( \"input\" TEXT , \"model_name\" TEXT DEFAULT 'sentence-transformers/all-MiniLM-L6-v2' , \"api_key\" TEXT DEFAULT NULL ) RETURNS double precision [] Parameters: Parameter Type Description input text Raw text to be transformed to an embedding model_name text Name of the sentence-transformer or OpenAI model to use. api_key text API key for the transformer. Defaults to NULL. Example \u00b6 select vectorize . encode ( input => 'the quick brown fox jumped over the lazy dogs' , model_name => 'sentence-transformers/multi-qa-MiniLM-L6-dot-v1' ); { - 0 . 2556323707103729 , - 0 . 3213586211204529 ..., - 0 . 0951206386089325 } Updating the Database \u00b6 Configure vectorize to run on a database other than the default postgres . Note that when making this change, it's also required to update pg_cron such that its corresponding background workers also connect to the appropriate database. Example \u00b6 CREATE DATABASE my_new_db ; ALTER SYSTEM SET cron . database_name TO 'my_new_db' ; ALTER SYSTEM SET vectorize . database_name TO 'my_new_db' ; Then, restart postgres to apply the changes and, if you haven't already, enable vectorize in your new database. \\ c my_new_db CREATE EXTENSION vectorize CASCADE ; SHOW cron . database_name ; SHOW vectorize . database_name ; cron.database_name -------------------- my_new_db (1 row) vectorize.database_name ------------------------- my_new_db (1 row)","title":"Utilities"},{"location":"api/utilities/#utilities","text":"","title":"Utilities"},{"location":"api/utilities/#text-to-embeddings","text":"Transforms a block of text to embeddings using the specified transformer. Requires the vector-serve container to be set via vectorize.embedding_service_url , or an OpenAI key to be set if using OpenAI embedding models. vectorize . \"encode\" ( \"input\" TEXT , \"model_name\" TEXT DEFAULT 'sentence-transformers/all-MiniLM-L6-v2' , \"api_key\" TEXT DEFAULT NULL ) RETURNS double precision [] Parameters: Parameter Type Description input text Raw text to be transformed to an embedding model_name text Name of the sentence-transformer or OpenAI model to use. api_key text API key for the transformer. Defaults to NULL.","title":"Text to Embeddings"},{"location":"api/utilities/#example","text":"select vectorize . encode ( input => 'the quick brown fox jumped over the lazy dogs' , model_name => 'sentence-transformers/multi-qa-MiniLM-L6-dot-v1' ); { - 0 . 2556323707103729 , - 0 . 3213586211204529 ..., - 0 . 0951206386089325 }","title":"Example"},{"location":"api/utilities/#updating-the-database","text":"Configure vectorize to run on a database other than the default postgres . Note that when making this change, it's also required to update pg_cron such that its corresponding background workers also connect to the appropriate database.","title":"Updating the Database"},{"location":"api/utilities/#example_1","text":"CREATE DATABASE my_new_db ; ALTER SYSTEM SET cron . database_name TO 'my_new_db' ; ALTER SYSTEM SET vectorize . database_name TO 'my_new_db' ; Then, restart postgres to apply the changes and, if you haven't already, enable vectorize in your new database. \\ c my_new_db CREATE EXTENSION vectorize CASCADE ; SHOW cron . database_name ; SHOW vectorize . database_name ; cron.database_name -------------------- my_new_db (1 row) vectorize.database_name ------------------------- my_new_db (1 row)","title":"Example"},{"location":"examples/openai_embeddings/","text":"Vector Search with OpenAI \u00b6 First you'll need an OpenAI API key . Set your API key as a Postgres configuration parameter. ALTER SYSTEM SET vectorize . openai_key TO '<your api key>' ; SELECT pg_reload_conf (); Create an example table if it does not already exist. CREATE TABLE products ( LIKE vectorize . example_products INCLUDING ALL ); INSERT INTO products SELECT * FROM vectorize . example_products ; Then create the job. It may take some time to generate embeddings, depending on API latency. SELECT vectorize . table ( job_name => 'product_search_openai' , relation => 'products' , primary_key => 'product_id' , columns => ARRAY [ 'product_name' , 'description' ], transformer => 'openai/text-embedding-ada-002' ); To search the table, use the vectorize.search function. SELECT * FROM vectorize . search ( job_name => 'product_search_openai' , query => 'accessories for mobile devices' , return_columns => ARRAY [ 'product_id' , 'product_name' ], num_results => 3 ); search_results -------------------------------------------------------------------------------------------- ---- {\"product_id\": 13, \"product_name\": \"Phone Charger\", \"similarity_score\": 0.8564681325237845} {\"product_id\": 24, \"product_name\": \"Tablet Holder\", \"similarity_score\": 0.8295988934993099} {\"product_id\": 4, \"product_name\": \"Bluetooth Speaker\", \"similarity_score\": 0.8250355616233103} (3 rows)","title":"Vector Search with OpenAI"},{"location":"examples/openai_embeddings/#vector-search-with-openai","text":"First you'll need an OpenAI API key . Set your API key as a Postgres configuration parameter. ALTER SYSTEM SET vectorize . openai_key TO '<your api key>' ; SELECT pg_reload_conf (); Create an example table if it does not already exist. CREATE TABLE products ( LIKE vectorize . example_products INCLUDING ALL ); INSERT INTO products SELECT * FROM vectorize . example_products ; Then create the job. It may take some time to generate embeddings, depending on API latency. SELECT vectorize . table ( job_name => 'product_search_openai' , relation => 'products' , primary_key => 'product_id' , columns => ARRAY [ 'product_name' , 'description' ], transformer => 'openai/text-embedding-ada-002' ); To search the table, use the vectorize.search function. SELECT * FROM vectorize . search ( job_name => 'product_search_openai' , query => 'accessories for mobile devices' , return_columns => ARRAY [ 'product_id' , 'product_name' ], num_results => 3 ); search_results -------------------------------------------------------------------------------------------- ---- {\"product_id\": 13, \"product_name\": \"Phone Charger\", \"similarity_score\": 0.8564681325237845} {\"product_id\": 24, \"product_name\": \"Tablet Holder\", \"similarity_score\": 0.8295988934993099} {\"product_id\": 4, \"product_name\": \"Bluetooth Speaker\", \"similarity_score\": 0.8250355616233103} (3 rows)","title":"Vector Search with OpenAI"},{"location":"examples/scheduling/","text":"Scheduling Embedding Updates \u00b6 When the source text data is updated, how and when the embeddings are updated is determined by the value set to the schedule parameter in vectorize.table s. The default behavior is schedule => '* * * * *' , which means the background worker process checks for changes every minute, and updates the embeddings accordingly. This method requires setting the updated_at_col value to point to a colum on the table indicating the time that the input text columns were last changed. schedule can be set to any cron-like value. Alternatively, schedule => 'realtime creates triggers on the source table and updates embeddings anytime new records are inserted to the source table or existing records are updated. Statements below would will result in new embeddings being generated either immediately ( schedule => 'realtime' ) or within the cron schedule set in the schedule parameter. INSERT INTO products ( product_id , product_name , description , product_category , price ) VALUES ( 12345 , 'pizza' , 'dish of Italian origin consisting of a flattened disk of bread' , 'food' , 5 . 99 ); UPDATE products SET description = 'sling made of fabric, rope, or netting, suspended between two or more points, used for swinging, sleeping, or resting' WHERE product_name = 'Hammock' ;","title":"Scheduling Embedding Updates"},{"location":"examples/scheduling/#scheduling-embedding-updates","text":"When the source text data is updated, how and when the embeddings are updated is determined by the value set to the schedule parameter in vectorize.table s. The default behavior is schedule => '* * * * *' , which means the background worker process checks for changes every minute, and updates the embeddings accordingly. This method requires setting the updated_at_col value to point to a colum on the table indicating the time that the input text columns were last changed. schedule can be set to any cron-like value. Alternatively, schedule => 'realtime creates triggers on the source table and updates embeddings anytime new records are inserted to the source table or existing records are updated. Statements below would will result in new embeddings being generated either immediately ( schedule => 'realtime' ) or within the cron schedule set in the schedule parameter. INSERT INTO products ( product_id , product_name , description , product_category , price ) VALUES ( 12345 , 'pizza' , 'dish of Italian origin consisting of a flattened disk of bread' , 'food' , 5 . 99 ); UPDATE products SET description = 'sling made of fabric, rope, or netting, suspended between two or more points, used for swinging, sleeping, or resting' WHERE product_name = 'Hammock' ;","title":"Scheduling Embedding Updates"},{"location":"examples/sentence_transformers/","text":"Sentence Transformers \u00b6 Setup a products table. Copy from the example data provided by the extension. Ensure vectorize.embedding_svc_url is set to the URL of the vector-serve container. If you're running this example using the docker-compose.yaml file from this repo, it should look like this: SHOW vectorize . embedding_service_url ; vectorize.embedding_service_url ---------------------------------------- http://vector-serve:3000/v1/embeddings (1 row) If you are not running in docker, then you will need to change the url to the appropriate location. If that is localhost, it would look like this; ALTER SYSTEM SET vectorize . embedding_svc_url TO 'http://localhost:3000/v1/embeddings' ; Then reload Postgres configurations: SELECT pg_reload_conf (); Create an example table if it does not already exist. CREATE TABLE products ( LIKE vectorize . example_products INCLUDING ALL ); INSERT INTO products SELECT * FROM vectorize . example_products ; SELECT * FROM products limit 2 ; product_id | product_name | description | last_updated_at ------------+--------------+--------------------------------------------------------+------------------------------- 1 | Pencil | Utensil used for writing and often works best on paper | 2023-07-26 17:20:43.639351-05 2 | Laptop Stand | Elevated platform for laptops, enhancing ergonomics | 2023-07-26 17:20:43.639351-05 Create a job to vectorize the products table. We'll specify the tables primary key (product_id) and the columns that we want to search (product_name and description). SELECT vectorize . table ( job_name => 'product_search_hf' , relation => 'products' , primary_key => 'product_id' , columns => ARRAY [ 'product_name' , 'description' ], transformer => 'sentence-transformers/multi-qa-MiniLM-L6-dot-v1' , scheduler => 'realtime' ); This adds a new column to your table, in our case it is named product_search_embeddings , then populates that data with the transformed embeddings from the product_name and description columns. Then search, SELECT * FROM vectorize . search ( job_name => 'product_search_hf' , query => 'accessories for mobile devices' , return_columns => ARRAY [ 'product_id' , 'product_name' ], num_results => 3 ); search_results --------------------------------------------------------------------------------------------- { \"product_id\" : 13 , \"product_name\" : \"Phone Charger\" , \"similarity_score\" : 0 . 8147814132322894 } { \"product_id\" : 6 , \"product_name\" : \"Backpack\" , \"similarity_score\" : 0 . 7743061352550308 } { \"product_id\" : 11 , \"product_name\" : \"Stylus Pen\" , \"similarity_score\" : 0 . 7709902653575383 }","title":"Sentence Transformers"},{"location":"examples/sentence_transformers/#sentence-transformers","text":"Setup a products table. Copy from the example data provided by the extension. Ensure vectorize.embedding_svc_url is set to the URL of the vector-serve container. If you're running this example using the docker-compose.yaml file from this repo, it should look like this: SHOW vectorize . embedding_service_url ; vectorize.embedding_service_url ---------------------------------------- http://vector-serve:3000/v1/embeddings (1 row) If you are not running in docker, then you will need to change the url to the appropriate location. If that is localhost, it would look like this; ALTER SYSTEM SET vectorize . embedding_svc_url TO 'http://localhost:3000/v1/embeddings' ; Then reload Postgres configurations: SELECT pg_reload_conf (); Create an example table if it does not already exist. CREATE TABLE products ( LIKE vectorize . example_products INCLUDING ALL ); INSERT INTO products SELECT * FROM vectorize . example_products ; SELECT * FROM products limit 2 ; product_id | product_name | description | last_updated_at ------------+--------------+--------------------------------------------------------+------------------------------- 1 | Pencil | Utensil used for writing and often works best on paper | 2023-07-26 17:20:43.639351-05 2 | Laptop Stand | Elevated platform for laptops, enhancing ergonomics | 2023-07-26 17:20:43.639351-05 Create a job to vectorize the products table. We'll specify the tables primary key (product_id) and the columns that we want to search (product_name and description). SELECT vectorize . table ( job_name => 'product_search_hf' , relation => 'products' , primary_key => 'product_id' , columns => ARRAY [ 'product_name' , 'description' ], transformer => 'sentence-transformers/multi-qa-MiniLM-L6-dot-v1' , scheduler => 'realtime' ); This adds a new column to your table, in our case it is named product_search_embeddings , then populates that data with the transformed embeddings from the product_name and description columns. Then search, SELECT * FROM vectorize . search ( job_name => 'product_search_hf' , query => 'accessories for mobile devices' , return_columns => ARRAY [ 'product_id' , 'product_name' ], num_results => 3 ); search_results --------------------------------------------------------------------------------------------- { \"product_id\" : 13 , \"product_name\" : \"Phone Charger\" , \"similarity_score\" : 0 . 8147814132322894 } { \"product_id\" : 6 , \"product_name\" : \"Backpack\" , \"similarity_score\" : 0 . 7743061352550308 } { \"product_id\" : 11 , \"product_name\" : \"Stylus Pen\" , \"similarity_score\" : 0 . 7709902653575383 }","title":"Sentence Transformers"},{"location":"models/","text":"Supported Transformers and Generative Models \u00b6 pg_vectorize provides hooks into two types of models; text-to-embedding transformer models and text-generation models. Whether a model is a text-to-embedding transformer or a text generation model, the models are always referenced from SQL using the following syntax: ${provider}/${model-name} A few illustrative examples: openai/text-embedding-ada-002 is one of OpenAI's earliest embedding models openai/gpt-3.5-turbo-instruct is a text generation model from OpenAI. ollama/wizardlm2:7b is a language model hosted in Ollama and developed by MicrosoftAI. sentence-transformers/all-MiniLM-L12-v2 is a text-to-embedding model from SentenceTransformers . Text-to-Embedding Models \u00b6 pg_vectorize provides hooks into the following tex-to-embedding models: OpenAI (public API) SentenceTransformers (self-hosted) The transformer model that you want to be used is specified in a parameter in various functions in this project, For example, the sentence-transformers provider has a model named all-MiniLM-L12-v2 . The model name is sentence-transformers/all-MiniLM-L12-v2 . To use openai's text-embedding-ada-002 , the model name is openai/text-embedding-ada-002 . SentenceTransformers \u00b6 SentenceTransformers is a Python library for computing text embeddings. pg_vectorize provides a container image that implements the SentenceTransformer library beyind a REST API. The container image is pre-built with sentence-transformers/all-MiniLM-L12-v2 pre-cached. Models that are not pre-cached will be downloaded on first use and cached for subsequent use. When calling the model server from Postgres, the url to the model server must first be set in the vectorize.embedding_service_url configuration parameter. Assuming the model server is running on the same host as Postgres, you would set the following: ALTER SYSTEM SET vectorize . embedding_service_url TO 'http://localhost:3000/v1/embeddings' ; SELECT pg_reload_conf (); Running the model server \u00b6 You can run this model server locally by executing docker compose up vector-serve -d Then call it with simple curl commands: Calling with curl \u00b6 curl -X POST http://localhost:3000/v1/embeddings \\ -H 'Content-Type: application/json' \\ -d '{\"input\": [\"solar powered mobile electronics accessories without screens\"], \"model\": \"sentence-transformers/all-MiniLM-L12-v2\"}' { \"data\": [ { \"embedding\": [ -0.07903402298688889, 0.028912536799907684, -0.018827738240361214, -0.013423092663288116, -0.06503172218799591, ....384 total elements ], \"index\": 0 } ], \"model\": \"all-MiniLM-L12-v2\" } We can change the model name to any of the models supported by SentenceTransformers, and it will be downloaded on-the-fly. curl -X POST http://localhost:3000/v1/embeddings \\ -H 'Content-Type: application/json' \\ -d '{\"input\": [\"solar powered mobile electronics accessories without screens\"], \"model\": \"sentence-transformers/sentence-t5-base\"}' { \"data\": [ { \"embedding\": [ -0.07903402298688889, 0.028912536799907684, -0.018827738240361214, -0.013423092663288116, -0.06503172218799591, ....384 total elements ], \"index\": 0 } ], \"model\": \"sentence-transformers/sentence-t5-base\" } Calling with SQL \u00b6 We can also call the model server from SQL using the pg_vectorize.transform_embeddings function. Model name support rules apply the same. select vectorize . transform_embeddings ( input => 'the quick brown fox jumped over the lazy dogs' , model_name => 'sentence-transformers/multi-qa-MiniLM-L6-dot-v1' ); {-0.2556323707103729,-0.3213586211204529 ..., -0.0951206386089325} OpenAI \u00b6 OpenAI embedding models are hosted by OpenAI's public API. You just need to have an API key of your own, and it can be set with: ALTER SYSTEM SET vectorize . openai_key TO '<your api key>' ; SELECT pg_reload_conf (); To call the text-embedding-ada-002 from OpenAI: select vectorize . transform_embeddings ( input => 'the quick brown fox jumped over the lazy dogs' , model_name => 'openai/text-embedding-ada-002' ); To call text-embedding-3-large select vectorize . transform_embeddings ( input => 'the quick brown fox jumped over the lazy dogs' , model_name => 'openai/text-embedding-3-large' ); Text Generation Models \u00b6 pg_vectorize provides hooks into the following text generation models: OpenAI (public API) Ollama (self-hosted) Ollama Generative Models \u00b6 To run the self-hosted Ollama models, you must first start the model server: docker compose up ollama-serve -d This starts an Ollama server pre-loaded with the wizardlm2:7b model. Calling with curl \u00b6 Once the Ollama server is running, you can call it directly with curl : curl http://localhost:3001/api/generate -d '{ \"model\": \"wizardlm2:7b\", \"prompt\": \"What is Postgres?\" }' Calling with SQL \u00b6 First set the url to the Ollama server: ALTER SYSTEM set vectorize . ollama_service_url TO ' http : // localhost : 3001 ` ; SELECT pg_reload_conf (); The text-generation models are available as part of the RAG API. To call the models provided by the self-hosted Ollama container, pass the model name into the chat_model parameter. SELECT vectorize . rag ( job_name => 'product_chat' , query => 'What is a pencil?' , chat_model => 'ollama/wizardlm2:7b' ); Loading new Ollama models \u00b6 While Ollama server comes preloaded with wizardlm2:7b , we can load and model supported by Ollama by calling the /api/pull endpoint. The service is compatible with all models available in the Ollama library . To pull Llama 3: curl http://localhost:3001/api/pull -d '{ \"name\": \"llama3\" }' Then use that model in your RAG application: SELECT vectorize . rag ( job_name => 'product_chat' , query => 'What is a pencil?' chat_model => 'ollama/llama3' );","title":"Model Providers"},{"location":"models/#supported-transformers-and-generative-models","text":"pg_vectorize provides hooks into two types of models; text-to-embedding transformer models and text-generation models. Whether a model is a text-to-embedding transformer or a text generation model, the models are always referenced from SQL using the following syntax: ${provider}/${model-name} A few illustrative examples: openai/text-embedding-ada-002 is one of OpenAI's earliest embedding models openai/gpt-3.5-turbo-instruct is a text generation model from OpenAI. ollama/wizardlm2:7b is a language model hosted in Ollama and developed by MicrosoftAI. sentence-transformers/all-MiniLM-L12-v2 is a text-to-embedding model from SentenceTransformers .","title":"Supported Transformers and Generative Models"},{"location":"models/#text-to-embedding-models","text":"pg_vectorize provides hooks into the following tex-to-embedding models: OpenAI (public API) SentenceTransformers (self-hosted) The transformer model that you want to be used is specified in a parameter in various functions in this project, For example, the sentence-transformers provider has a model named all-MiniLM-L12-v2 . The model name is sentence-transformers/all-MiniLM-L12-v2 . To use openai's text-embedding-ada-002 , the model name is openai/text-embedding-ada-002 .","title":"Text-to-Embedding Models"},{"location":"models/#sentencetransformers","text":"SentenceTransformers is a Python library for computing text embeddings. pg_vectorize provides a container image that implements the SentenceTransformer library beyind a REST API. The container image is pre-built with sentence-transformers/all-MiniLM-L12-v2 pre-cached. Models that are not pre-cached will be downloaded on first use and cached for subsequent use. When calling the model server from Postgres, the url to the model server must first be set in the vectorize.embedding_service_url configuration parameter. Assuming the model server is running on the same host as Postgres, you would set the following: ALTER SYSTEM SET vectorize . embedding_service_url TO 'http://localhost:3000/v1/embeddings' ; SELECT pg_reload_conf ();","title":"SentenceTransformers"},{"location":"models/#running-the-model-server","text":"You can run this model server locally by executing docker compose up vector-serve -d Then call it with simple curl commands:","title":"Running the model server"},{"location":"models/#calling-with-curl","text":"curl -X POST http://localhost:3000/v1/embeddings \\ -H 'Content-Type: application/json' \\ -d '{\"input\": [\"solar powered mobile electronics accessories without screens\"], \"model\": \"sentence-transformers/all-MiniLM-L12-v2\"}' { \"data\": [ { \"embedding\": [ -0.07903402298688889, 0.028912536799907684, -0.018827738240361214, -0.013423092663288116, -0.06503172218799591, ....384 total elements ], \"index\": 0 } ], \"model\": \"all-MiniLM-L12-v2\" } We can change the model name to any of the models supported by SentenceTransformers, and it will be downloaded on-the-fly. curl -X POST http://localhost:3000/v1/embeddings \\ -H 'Content-Type: application/json' \\ -d '{\"input\": [\"solar powered mobile electronics accessories without screens\"], \"model\": \"sentence-transformers/sentence-t5-base\"}' { \"data\": [ { \"embedding\": [ -0.07903402298688889, 0.028912536799907684, -0.018827738240361214, -0.013423092663288116, -0.06503172218799591, ....384 total elements ], \"index\": 0 } ], \"model\": \"sentence-transformers/sentence-t5-base\" }","title":"Calling with curl"},{"location":"models/#calling-with-sql","text":"We can also call the model server from SQL using the pg_vectorize.transform_embeddings function. Model name support rules apply the same. select vectorize . transform_embeddings ( input => 'the quick brown fox jumped over the lazy dogs' , model_name => 'sentence-transformers/multi-qa-MiniLM-L6-dot-v1' ); {-0.2556323707103729,-0.3213586211204529 ..., -0.0951206386089325}","title":"Calling with SQL"},{"location":"models/#openai","text":"OpenAI embedding models are hosted by OpenAI's public API. You just need to have an API key of your own, and it can be set with: ALTER SYSTEM SET vectorize . openai_key TO '<your api key>' ; SELECT pg_reload_conf (); To call the text-embedding-ada-002 from OpenAI: select vectorize . transform_embeddings ( input => 'the quick brown fox jumped over the lazy dogs' , model_name => 'openai/text-embedding-ada-002' ); To call text-embedding-3-large select vectorize . transform_embeddings ( input => 'the quick brown fox jumped over the lazy dogs' , model_name => 'openai/text-embedding-3-large' );","title":"OpenAI"},{"location":"models/#text-generation-models","text":"pg_vectorize provides hooks into the following text generation models: OpenAI (public API) Ollama (self-hosted)","title":"Text Generation Models"},{"location":"models/#ollama-generative-models","text":"To run the self-hosted Ollama models, you must first start the model server: docker compose up ollama-serve -d This starts an Ollama server pre-loaded with the wizardlm2:7b model.","title":"Ollama Generative Models"},{"location":"models/#calling-with-curl_1","text":"Once the Ollama server is running, you can call it directly with curl : curl http://localhost:3001/api/generate -d '{ \"model\": \"wizardlm2:7b\", \"prompt\": \"What is Postgres?\" }'","title":"Calling with curl"},{"location":"models/#calling-with-sql_1","text":"First set the url to the Ollama server: ALTER SYSTEM set vectorize . ollama_service_url TO ' http : // localhost : 3001 ` ; SELECT pg_reload_conf (); The text-generation models are available as part of the RAG API. To call the models provided by the self-hosted Ollama container, pass the model name into the chat_model parameter. SELECT vectorize . rag ( job_name => 'product_chat' , query => 'What is a pencil?' , chat_model => 'ollama/wizardlm2:7b' );","title":"Calling with SQL"},{"location":"models/#loading-new-ollama-models","text":"While Ollama server comes preloaded with wizardlm2:7b , we can load and model supported by Ollama by calling the /api/pull endpoint. The service is compatible with all models available in the Ollama library . To pull Llama 3: curl http://localhost:3001/api/pull -d '{ \"name\": \"llama3\" }' Then use that model in your RAG application: SELECT vectorize . rag ( job_name => 'product_chat' , query => 'What is a pencil?' chat_model => 'ollama/llama3' );","title":"Loading new Ollama models"}]}